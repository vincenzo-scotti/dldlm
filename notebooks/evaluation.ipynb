{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('./src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel\n",
    "from joblib import delayed\n",
    "from joblib import parallel_backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.metrics import (\n",
    "    homogeneity_score,\n",
    "    completeness_score,\n",
    "    v_measure_score,\n",
    "    silhouette_score,\n",
    "    calinski_harabasz_score,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vincenzoscotti_polimi/anaconda3/envs/dldlm/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants and global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLITS = ('train', 'validation', 'test')\n",
    "MODELS = ('Pretrained', 'Fine-Tuned')\n",
    "DATA_SETS = ('DailyDialog', 'EmpatheticDialogues', 'Persona-Chat', 'Wizard of Wikipedia', 'HOPE')\n",
    "DISTRIBUTIONS = ('latent_prior_dist', 'latent_posterior_dist')\n",
    "HIDDEN_VECTORS = ('prior_hidden_state', 'posterior_hidden_state')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '../resources/data/cache/'\n",
    "PROCESSED_DATA_PATHS = (\n",
    "    '../experiments/DLDLM_evaluation/dldlm_large_nllt_lda_lr_2022_11_10_22_41_08/data', \n",
    "    '../experiments/DLDLM_evaluation/dldlm_large_nllt_gibbs_2022_11_11_09_54_42/data'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_LATENTS = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_REPETITIONS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "DISTRIBUTION_MAP = {'latent_prior_dist': 'Prior', 'latent_posterior_dist': 'Posterior'}\n",
    "DIALOGUE_ACT_MAP = {\n",
    "    'Clarification Request': 'Clarification Request',\n",
    "    'Opinion Request': 'Opinion Request',\n",
    "    'Information Request': 'Information Request',\n",
    "    'Yes/No question': 'Yes/No Question',\n",
    "    'Greeting': 'Greeting',\n",
    "    'Acknowledgement': 'Acknowledgement',\n",
    "    'General Chat': 'General Chat',\n",
    "    'Information Delivery': 'Information Delivery',\n",
    "    'Positive Answer': 'Positive Answer',\n",
    "    'Clarification Delivery': 'Clarification Delivery',\n",
    "    'Negative Answer': 'Negative Answer',\n",
    "    'Opinion Delivery': 'Opinion Delivery'\n",
    "}\n",
    "DIALOGUE_ACT_TYPE_MAP = {\n",
    "    'Clarification Request': 'Speaker Initiative',\n",
    "    'Opinion Request': 'Speaker Initiative',\n",
    "    'Information Request': 'Speaker Initiative',\n",
    "    'Yes/No question': 'Speaker Initiative',\n",
    "    'Greeting': 'General',\n",
    "    'Acknowledgement': 'General',\n",
    "    'General Chat': 'General',\n",
    "    'Information Delivery': 'Speaker Responsive',\n",
    "    'Positive Answer': 'Speaker Responsive',\n",
    "    'Clarification Delivery': 'Speaker Responsive',\n",
    "    'Negative Answer': 'Speaker Responsive',\n",
    "    'Opinion Delivery': 'Speaker Responsive'\n",
    "}\n",
    "SENTIMENT_MAP = {'Very negative': 'Very Negative', 'Negative': 'Negative', 'Neutral': 'Neutral', 'Positive': 'Positive', 'Very Positive': 'Very Positive'}\n",
    "SPEAKER_MAP = {'Therapist': 'Therapist', 'Patient': 'Patient'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_WORDS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIALOGUE_ACT_ENCODER = {value: i for i, value in enumerate(DIALOGUE_ACT_MAP.values())}\n",
    "DIALOGUE_ACT_TYPE_ENCODER = {value: i for i, value in enumerate(DIALOGUE_ACT_TYPE_MAP.values())}\n",
    "SENTIMENT_ENCODER = {value: i for i, value in enumerate(SENTIMENT_MAP.values())}\n",
    "SPEAKER_ENCODER = {value: i for i, value in enumerate(SPEAKER_MAP.values())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 2307"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fda726666f0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    # Open the BZIP2 file\n",
    "    with bz2.BZ2File(path, 'r') as f:\n",
    "        # Read the content with Pickle\n",
    "        data = pickle.load(f)\n",
    "    # Return loaded data\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_by(data, group):\n",
    "    # Initialise a dictionary to store the groups\n",
    "    grouped_data = {value: [] for value in {sample[group] for sample in data}}\n",
    "    # Go through the data set and add each sample to the correct group\n",
    "    for sample in data:\n",
    "        if sample[group] in grouped_data:\n",
    "            grouped_data[sample[group]].append(sample)\n",
    "        else:\n",
    "            grouped_data[sample[group]]= [sample]\n",
    "    # Return grouped data\n",
    "    return grouped_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metadata(data):\n",
    "    # Helper function to compute the desired info on a single sample\n",
    "    def get_metadata(sample):\n",
    "        return {'corpus': sample['corpus'], 'conversation_idx': sample['conversation_idx'], 'n_tokens': len(word_tokenize(sample['response']))}\n",
    "    # Define parallel backend to compute the desired metadata\n",
    "    with parallel_backend('threading', n_jobs=-1):\n",
    "        # Compute No. of tokens for each turn of each dialogue in each corpus (mantaing the splitting info)\n",
    "        metadata = Parallel(verbose=0)(delayed(get_metadata)(sample) for sample in data)\n",
    "    # Return computed metadata\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stats(data):\n",
    "    # Group data by dialogue identifier\n",
    "    grouped_data = group_by(data, 'conversation_idx')\n",
    "    # Compute number of different dialogues\n",
    "    n_dialogues = len(grouped_data)\n",
    "    # Compute mean and standard deviation of dialogues length\n",
    "    mean_n_turns = mean(len(group) for group in grouped_data.values())\n",
    "    std_n_turns = stdev(len(group) for group in grouped_data.values())\n",
    "    # Compute mean and standard deviation of tokens per turn length\n",
    "    mean_n_tokens = mean(sample['n_tokens'] for sample in data)\n",
    "    std_n_tokens = stdev(sample['n_tokens'] for sample in data)\n",
    "    # Return computed statistics\n",
    "    return n_dialogues, (mean_n_turns, std_n_turns), (mean_n_tokens, std_n_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_same_sample(sample_a, sample_b):\n",
    "    return sample_a['corpus'] == sample_b['corpus'] and sample_a['conversation_idx'] == sample_b['conversation_idx'] and sample_a['turn_idx'] == sample_b['turn_idx']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    split: load_data(os.path.join(DATA_PATH, f'evaluation_corpus_{split}.pbz2')) for split in SPLITS\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = {\n",
    "    model: {split: load_data(os.path.join(data_path, f'evaluation_output_{split}.pbz2')) for split in SPLITS} \n",
    "    for model, data_path in zip(MODELS, PROCESSED_DATA_PATHS)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert all(\n",
    "    is_same_sample(s1, s2) and is_same_sample(s2, s3) \n",
    "    for s1, s2, s3 in zip(\n",
    "        data['validation'], \n",
    "        processed_data['Pretrained']['validation'], \n",
    "        processed_data['Fine-Tuned']['validation']\n",
    "    )\n",
    ")\n",
    "assert all(\n",
    "    is_same_sample(s1, s2) and is_same_sample(s2, s3) \n",
    "    for s1, s2, s3 in zip(\n",
    "        data['test'], \n",
    "        processed_data['Pretrained']['test'], \n",
    "        processed_data['Fine-Tuned']['test']\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = CountVectorizer(\n",
    "    max_df=0.95, \n",
    "    min_df=2, \n",
    "    stop_words='english'\n",
    ").fit([sample['response'] for sample in data['train'] if sample['corpus'] != 'HOPE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(\n",
    "    # tokenizer=word_tokenize, \n",
    "    # stop_words=set(stopwords.words('english')) | set(string.punctuation), \n",
    "    max_df=0.9, \n",
    "    min_df=2,\n",
    "    stop_words='english'\n",
    ").fit([sample['response'] for sample in data['train']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(\n",
    "    n_components=N_LATENTS,\n",
    "    learning_method='online',\n",
    "    learning_offset=50.0,\n",
    "    random_state=RANDOM_SEED,\n",
    ").fit(tf.transform([sample['response'] for sample in data['train'] if sample['corpus'] != 'HOPE']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Latent model analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entire data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data = {\n",
    "    split: data[split] for split in data if split != 'train'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Clusters evaluation\n",
    "\n",
    "We sample the latent ID $n$ from the prior and posterior distributions and we compute the usual clustering metrics, then we average the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = {split: tfidf.transform([sample['response'] for sample in eval_data[split]]) for split in eval_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# H = {\n",
    "#     split: {\n",
    "#             model: {\n",
    "#             DISTRIBUTION_MAP[distribution]: np.array([processed_data[model][split][hidden_state]])\n",
    "#             for distribution, hidden_state in zip(DISTRIBUTIONS, HIDDEN_VECTORS)\n",
    "#         } \n",
    "#         for model in MODELS\n",
    "#     }\n",
    "#     for split in eval_data\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_p = {\n",
    "    split: {\n",
    "            model: {\n",
    "            DISTRIBUTION_MAP[distribution]: torch.tensor(\n",
    "                [sample[distribution] for sample in processed_data[model][split]]\n",
    "            ).log() \n",
    "            for distribution in DISTRIBUTIONS\n",
    "        } \n",
    "        for model in MODELS\n",
    "    }\n",
    "    for split in eval_data\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_p_lda = {\n",
    "    split: {\n",
    "        'LDA': {\n",
    "            'Prior': torch.tensor([[1 / N_LATENTS] * N_LATENTS] * len(eval_data[split])).log(),\n",
    "            'Posterior': torch.tensor(lda.transform(tf.transform([sample['response'] for sample in eval_data[split]]))).log()\n",
    "        } \n",
    "    }\n",
    "    for split in eval_data\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperatures = [0.0, 0.333, 0.667, 0.9, 1.0, 1.5, 2.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "intrinsic_metrics = {'Silhouette': silhouette_score, 'Calinski-Harabasz index': calinski_harabasz_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_latents = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in temperatures:\n",
    "    for k in range(N_REPETITIONS):\n",
    "        # Sample latent\n",
    "        c = {\n",
    "            split: {\n",
    "                model: {\n",
    "                    distribution: (\n",
    "                        torch.multinomial(torch.softmax(log_probs / t, dim=-1), 1).squeeze().numpy()\n",
    "                        if t > 0 else torch.argmax(log_probs, dim=-1).squeeze().numpy()\n",
    "                    )\n",
    "                    for distribution, log_probs in distribution_dict.items()\n",
    "                } \n",
    "                for model, distribution_dict in model_dict.items()\n",
    "            } \n",
    "            for split, model_dict in log_p.items()\n",
    "        }\n",
    "        # Add sampling with LDA to have a term of comparison\n",
    "        c_lda = {\n",
    "            split: {\n",
    "                model: {\n",
    "                    distribution: (\n",
    "                        torch.multinomial(torch.softmax(log_probs / t, dim=-1), 1).squeeze().numpy()\n",
    "                        if t > 0 else torch.argmax(log_probs, dim=-1).squeeze().numpy()\n",
    "                    )\n",
    "                    for distribution, log_probs in distribution_dict.items()\n",
    "                } \n",
    "                for model, distribution_dict in model_dict.items()\n",
    "            } \n",
    "            for split, model_dict in log_p_lda.items()\n",
    "        }\n",
    "        # Store sampled latents\n",
    "        for split in c:\n",
    "            for model in c[split]:\n",
    "                for distribution in c[split][model]:\n",
    "                    for latent, count in zip(*np.unique(c[split][model][distribution], return_counts=True)):\n",
    "                        sampled_latents.append((t, k, model, distribution, latent, count, split.capitalize()))\n",
    "        # Intrinsic metrics\n",
    "        for metric, f in intrinsic_metrics.items():\n",
    "            for split in ['validation', 'test']:\n",
    "                for model, model_dict in c[split].items():\n",
    "                    for distribution, latent in model_dict.items():\n",
    "                        values.append((t, k, metric, f(X[split].toarray(), latent), model, distribution, None, 'TF-IDF', split))\n",
    "                        # values.append((t, k, metric, f(H[split][model][distribution], latent), model, distribution, None, 'Last hidden state', split))\n",
    "                for model, model_dict in c_lda[split].items():\n",
    "                    for distribution, latent in model_dict.items():\n",
    "                        values.append((t, k, metric, f(X[split].toarray(), latent), model, distribution, None, 'TF-IDF', split))\n",
    "                        # values.append((t, k, metric, f(H[split][model][distribution], latent), model, distribution, None, 'Last hidden state', split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval = pd.DataFrame(values, columns=['Sampling temperature', 'Fold', 'Metric', 'Score', 'Model', 'Distribution', 'Label', 'Vector','Split'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df_eval = pd.DataFrame(sampled_latents, columns=['Sampling temperature', 'Fold', 'Model', 'Distribution', 'Latent', 'Counts', 'Split'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the averaged distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in sample_df_eval['Split'].unique():\n",
    "    tmp_df = sample_df_eval[sample_df_eval['Split'] == split]\n",
    "    \n",
    "    fig, axes = plt.subplots(\n",
    "        nrows=len(tmp_df['Distribution'].unique()), \n",
    "        ncols=len(tmp_df['Sampling temperature'].unique()), \n",
    "        figsize=(18, 6), \n",
    "        sharex=True, \n",
    "        sharey=True\n",
    "    )\n",
    "\n",
    "    for i, (distribution, distribution_df) in enumerate(tmp_df.groupby('Distribution', sort=False)):\n",
    "        for j, (t, temperature_df) in enumerate(distribution_df.groupby('Sampling temperature', sort=False)):\n",
    "            sns.barplot(data=temperature_df, hue='Model', x='Latent', y='Counts', ax=axes[i][j], linewidth=1., edgecolor='0', order=[*range(N_LATENTS)])\n",
    "            axes[i][j].set_title(f'Split: {split},\\nDistribution: {distribution},\\nSampling temperature: {t:.3f}')\n",
    "            # axes[i][j].set_xlabel('Latent')\n",
    "            # axes[i][j].set_ylabel('Counts')\n",
    "            # axes[0][i].set_xlim(0.9, tmp_df_coarse[0].max() + 1000)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intrinsic evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_df = df_eval[df_eval['Metric'].isin(intrinsic_metrics)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(\n",
    "    data=tmp_df[tmp_df['Split'] == 'validation'], \n",
    "    x='Sampling temperature', \n",
    "    y='Score', \n",
    "    style='Distribution', \n",
    "    row='Vector',\n",
    "    col='Metric', \n",
    "    hue='Model', \n",
    "    kind='line'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(\n",
    "    data=tmp_df[tmp_df['Split'] == 'test'], \n",
    "    x='Sampling temperature', \n",
    "    y='Score', \n",
    "    style='Distribution', \n",
    "    row='Vector',\n",
    "    col='Metric',  \n",
    "    hue='Model', \n",
    "    kind='line'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Psychotherapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We focus the anlysis on the labels for the HOPE corpus, for multiple reasons:\n",
    "- It is the target data set\n",
    "- We have labels so we can appl both an extrinsic and intrinsic evaluation\n",
    "- Its size allows for repeated sampling validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hope = {\n",
    "    split: [\n",
    "        sample \n",
    "        for sample in data[split] \n",
    "        if sample['corpus'] == 'HOPE' and sample['speaker'] is not None and sample['dialogue_act'] is not None\n",
    "    ]\n",
    "    for split in data\n",
    "    if split != 'train'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Clusters evaluation\n",
    "\n",
    "We sample the latent ID $n$ from the prior and posterior distributions and we compute the usual clustering metrics, then we average the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = {split: tfidf.transform([sample['response'] for sample in hope[split]]) for split in hope}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = {\n",
    "    split: {\n",
    "        'Dialogue act': np.array([DIALOGUE_ACT_ENCODER[DIALOGUE_ACT_MAP[sample['dialogue_act']]] for sample in hope[split]]), \n",
    "        'Dialogue act type': np.array([DIALOGUE_ACT_TYPE_ENCODER[DIALOGUE_ACT_TYPE_MAP[sample['dialogue_act']]] for sample in hope[split]]), \n",
    "        'Speaker': np.array([SPEAKER_ENCODER[SPEAKER_MAP[sample['speaker']]] for sample in hope[split]]), \n",
    "        'Sentiment': np.array([SENTIMENT_ENCODER[SENTIMENT_MAP[sample['sentiment']]] for sample in hope[split]])\n",
    "    } for split in hope\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# H = {\n",
    "#     split: {\n",
    "#             model: {\n",
    "#             DISTRIBUTION_MAP[distribution]: np.array([processed_data[model][split][hidden_state]])\n",
    "#             for distribution, hidden_state in zip(DISTRIBUTIONS, HIDDEN_VECTORS)\n",
    "#         } \n",
    "#         for model in MODELS\n",
    "#     }\n",
    "#     for split in y\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_p = {\n",
    "    split: {\n",
    "            model: {\n",
    "            DISTRIBUTION_MAP[distribution]: torch.tensor(\n",
    "                [sample[distribution] for sample in processed_data[model][split] if sample['corpus'] == 'HOPE' and sample['speaker'] is not None and sample['dialogue_act'] is not None]\n",
    "            ).log() \n",
    "            for distribution in DISTRIBUTIONS\n",
    "        } \n",
    "        for model in MODELS\n",
    "    }\n",
    "    for split in y\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_p_lda = {\n",
    "    split: {\n",
    "        'LDA': {\n",
    "            'Prior': torch.tensor([[1 / N_LATENTS] * N_LATENTS] * len(hope[split])).log(),\n",
    "            'Posterior': torch.tensor(lda.transform(tf.transform([sample['response'] for sample in hope[split]]))).log()\n",
    "        } \n",
    "    }\n",
    "    for split in hope\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperatures = [0.0, 0.333, 0.667, 0.9, 1.0, 1.5, 2.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extrinsic_metrics = {'Homogeneity': homogeneity_score, 'Completeness': completeness_score, 'V-Measure': v_measure_score}\n",
    "intrinsic_metrics = {'Silhouette': silhouette_score, 'Calinski-Harabasz index': calinski_harabasz_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_latents = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in temperatures:\n",
    "    for k in range(N_REPETITIONS):\n",
    "        # Sample latent\n",
    "        c = {\n",
    "            split: {\n",
    "                model: {\n",
    "                    distribution: (\n",
    "                        torch.multinomial(torch.softmax(log_probs / t, dim=-1), 1).squeeze().numpy()\n",
    "                        if t > 0 else torch.argmax(log_probs, dim=-1).squeeze().numpy()\n",
    "                    )\n",
    "                    for distribution, log_probs in distribution_dict.items()\n",
    "                } \n",
    "                for model, distribution_dict in model_dict.items()\n",
    "            } \n",
    "            for split, model_dict in log_p.items()\n",
    "        }\n",
    "        # Add sampling with LDA to have a term of comparison\n",
    "        c_lda = {\n",
    "            split: {\n",
    "                model: {\n",
    "                    distribution: (\n",
    "                        torch.multinomial(torch.softmax(log_probs / t, dim=-1), 1).squeeze().numpy()\n",
    "                        if t > 0 else torch.argmax(log_probs, dim=-1).squeeze().numpy()\n",
    "                    )\n",
    "                    for distribution, log_probs in distribution_dict.items()\n",
    "                } \n",
    "                for model, distribution_dict in model_dict.items()\n",
    "            } \n",
    "            for split, model_dict in log_p_lda.items()\n",
    "        }\n",
    "        # Store sampled latents\n",
    "        for split in c:\n",
    "            for model in c[split]:\n",
    "                for distribution in c[split][model]:\n",
    "                    for latent, count in zip(*np.unique(c[split][model][distribution], return_counts=True)):\n",
    "                        sampled_latents.append((t, k, model, distribution, latent, count, split.capitalize()))\n",
    "        # Extrinsic metrics\n",
    "        for metric, f in extrinsic_metrics.items():\n",
    "            for split in ['validation', 'test']:\n",
    "                for model, model_dict in c[split].items():\n",
    "                    for distribution, latent in model_dict.items():\n",
    "                        for labelling, label in y[split].items():\n",
    "                            values.append((t, k, metric, f(label, latent), model, distribution, labelling, None, split))\n",
    "                for model, model_dict in c_lda[split].items():\n",
    "                    for distribution, latent in model_dict.items():\n",
    "                        for labelling, label in y[split].items():\n",
    "                            values.append((t, k, metric, f(label, latent), model, distribution, labelling, None, split))\n",
    "        # Intrinsic metrics\n",
    "        for metric, f in intrinsic_metrics.items():\n",
    "            for split in ['validation', 'test']:\n",
    "                for model, model_dict in c[split].items():\n",
    "                    for distribution, latent in model_dict.items():\n",
    "                        values.append((t, k, metric, f(X[split].toarray(), latent), model, distribution, None, 'TF-IDF', split))\n",
    "                        # values.append((t, k, metric, f(H[split][model][distribution], latent), model, distribution, None, 'Last hidden state', split))\n",
    "                for model, model_dict in c_lda[split].items():\n",
    "                    for distribution, latent in model_dict.items():\n",
    "                        values.append((t, k, metric, f(X[split].toarray(), latent), model, distribution, None, 'TF-IDF', split))\n",
    "                        # values.append((t, k, metric, f(H[split][model][distribution], latent), model, distribution, None, 'Last hidden state', split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(values, columns=['Sampling temperature', 'Fold', 'Metric', 'Score', 'Model', 'Distribution', 'Label', 'Vector','Split'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = pd.DataFrame(sampled_latents, columns=['Sampling temperature', 'Fold', 'Model', 'Distribution', 'Latent', 'Counts', 'Split'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the averaged distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in sample_df['Split'].unique():\n",
    "    tmp_df = sample_df[sample_df['Split'] == split]\n",
    "    \n",
    "    fig, axes = plt.subplots(\n",
    "        nrows=len(tmp_df['Distribution'].unique()), \n",
    "        ncols=len(tmp_df['Sampling temperature'].unique()), \n",
    "        figsize=(18, 6), \n",
    "        sharex=True, \n",
    "        sharey=True\n",
    "    )\n",
    "\n",
    "    for i, (distribution, distribution_df) in enumerate(tmp_df.groupby('Distribution', sort=False)):\n",
    "        for j, (t, temperature_df) in enumerate(distribution_df.groupby('Sampling temperature', sort=False)):\n",
    "            sns.barplot(data=temperature_df, hue='Model', x='Latent', y='Counts', ax=axes[i][j], linewidth=1., edgecolor='0', order=[*range(N_LATENTS)])\n",
    "            axes[i][j].set_title(f'Split: {split},\\nDistribution: {distribution},\\nSampling temperature: {t:.3f}')\n",
    "            # axes[i][j].set_xlabel('Latent')\n",
    "            # axes[i][j].set_ylabel('Counts')\n",
    "            # axes[0][i].set_xlim(0.9, tmp_df_coarse[0].max() + 1000)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Extrinsic evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_df = df[df['Metric'].isin(extrinsic_metrics)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.relplot(\n",
    "    data=tmp_df[tmp_df['Split'] == 'validation'], \n",
    "    x='Sampling temperature', \n",
    "    y='Score', \n",
    "    style='Distribution', \n",
    "    row='Label', \n",
    "    col='Metric', \n",
    "    hue='Model', \n",
    "    kind='line'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(\n",
    "    data=tmp_df[tmp_df['Split'] == 'test'], \n",
    "    x='Sampling temperature', \n",
    "    y='Score', \n",
    "    style='Distribution', \n",
    "    row='Label', \n",
    "    col='Metric', \n",
    "    hue='Model', \n",
    "    kind='line'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intrinsic evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_df = df[df['Metric'].isin(intrinsic_metrics)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(\n",
    "    data=tmp_df[tmp_df['Split'] == 'validation'], \n",
    "    x='Sampling temperature', \n",
    "    y='Score', \n",
    "    style='Distribution', \n",
    "    row='Vector',\n",
    "    col='Metric', \n",
    "    hue='Model', \n",
    "    kind='line'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(\n",
    "    data=tmp_df[tmp_df['Split'] == 'test'], \n",
    "    x='Sampling temperature', \n",
    "    y='Score', \n",
    "    style='Distribution', \n",
    "    row='Vector',\n",
    "    col='Metric',  \n",
    "    hue='Model', \n",
    "    kind='line'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Terms distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = {split: \n",
    "     {\n",
    "         speaker: tfidf.transform(\n",
    "             [sample['response'] for sample in samples]\n",
    "         )\n",
    "         for speaker, samples in group_by([s for s in hope[split] if s['corpus'] == 'HOPE' and s['speaker'] is not None and s['dialogue_act'] is not None], 'speaker').items()\n",
    "     }\n",
    "     for split in hope\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hope_data_vectors = {\n",
    "    split: {\n",
    "        latent: {\n",
    "            speaker: {\n",
    "                DISTRIBUTION_MAP[distribution]: (X[split][speaker].toarray() * np.array([sample[distribution][latent] for sample in samples]).reshape(-1, 1))\n",
    "                for distribution in DISTRIBUTIONS\n",
    "            }\n",
    "            for speaker, samples in group_by([s for s in samples if s['corpus'] == 'HOPE' and s['speaker'] is not None and s['dialogue_act'] is not None], 'speaker').items()\n",
    "        }\n",
    "        for latent in range(N_LATENTS)\n",
    "    } \n",
    "    for split, samples in processed_data['Fine-Tuned'].items() if split != 'train'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (split, split_data) in enumerate(hope_data_vectors.items()):\n",
    "    fig, axes = plt.subplots(nrows=16, ncols=6, figsize=(24, 40))\n",
    "    for j, (latent, latent_data) in enumerate(split_data.items()):\n",
    "        Z = \n",
    "        for k, (speaker, speaker_data) in enumerate(latent_data.items()):\n",
    "            for d, (distribution, disribution_data) in enumerate(speaker_data.items())\n",
    "                x = np.asarray(speaker_data.sum(0))[0]\n",
    "                if distribution in Z:\n",
    "                    Z[distribution].append(x.copy())\n",
    "                else:\n",
    "                    Z[distribution] = [x.copy()]\n",
    "                idxs = x.argsort()[: -TOP_WORDS - 1 : -1][::-1]\n",
    "                x = x[idxs]\n",
    "                y = vocabulary[idxs]\n",
    "                axes[j][(3 * d) + k].barh(y, x, linewidth=1., edgecolor='0')\n",
    "                axes[j][(3 * d) + k].set_title(f'Split: {split.capitalize()},\\nDialogue act: {action},\\nDistribution: {distribution},\\nSpeaker: {speaker}')\n",
    "                axes[j][(3 * d) + k].set_xlabel('Cumulative TF-IDF')\n",
    "                axes[j][(3 * d) + k].set_ylabel('Word')\n",
    "        for d, (distribution, distribution_data) in enumerate(Z.items())\n",
    "            x = sum(distribution_data)\n",
    "            idxs = x.argsort()[: -TOP_WORDS - 1 : -1][::-1]\n",
    "            x = x[idxs]\n",
    "            y = vocabulary[idxs]\n",
    "            axes[j][(3 * d) - 1].barh(y, x, linewidth=1., edgecolor='0', color='tab:red')\n",
    "            axes[j][(3 * d) - 1].set_title(f'Split: {split.capitalize()},\\nLatent: {action},\\nDistribution: {distribution}\\n')\n",
    "            axes[j][(3 * d) - 1].set_xlabel('Cumulative TF-IDF')\n",
    "            axes[j][(3 * d) - 1].set_ylabel('Word')\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
