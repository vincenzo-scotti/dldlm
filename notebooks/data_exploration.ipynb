{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Data exploration"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vincenzoscotti_polimi/anaconda3/envs/dldlm/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/vincenzoscotti_polimi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/vincenzoscotti_polimi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from src.model import DLDLMTokenizer\n",
    "from src.data import DLDLMCorpus\n",
    "\n",
    "from typing import Tuple, Dict, List"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'DLDLMTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "tokenizer: DLDLMTokenizer = DLDLMTokenizer.from_pretrained('gpt2').extend_from_gpt2_tokenizer(0)\n",
    "\n",
    "splits: Tuple[str, str, str] = ('train', 'validation', 'test')\n",
    "corpus_splits: Dict[str, DLDLMCorpus] = {\n",
    "    split: DLDLMCorpus(\n",
    "        '../resources/data/raw/', tokenizer, split, '../resources/data/cache/',\n",
    "        corpus_list=['dailydialog', 'empatheticdialogues', 'personachat', 'wizard_of_wikipedia'],\n",
    "        max_context_length=256,\n",
    "        max_response_length=128,\n",
    "        count_word_tokens=True\n",
    "    )\n",
    "    for split in splits\n",
    "}\n",
    "\n",
    "top_n = 30"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get most common words per split\n",
    "counts: Dict[str, Counter] = {\n",
    "    split: sum((corpus_splits[split][i]['word_counts'] for i in range(len(corpus_splits[split]))), Counter())\n",
    "    for split in splits\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot word counts\n",
    "figure, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 20), sharex=True)\n",
    "axes = axes.flatten()\n",
    "for key_idx, split in enumerate(counts):\n",
    "    items, occurrences = list(zip(*counts[split].most_common(top_n)))\n",
    "    ax = axes[key_idx]\n",
    "    ax.barh(items, counts, height=0.7)\n",
    "    ax.set_title(f\"Split: {split.capitalize()}\", fontdict={\"fontsize\": 20})\n",
    "    ax.invert_yaxis()\n",
    "    ax.tick_params(axis=\"both\", which=\"major\", labelsize=20)\n",
    "    for i in \"top right left\".split():\n",
    "        ax.spines[i].set_visible(False)\n",
    "    figure.suptitle(f\"Split-wise most common words\", fontsize=32)\n",
    "plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot sorted word distribution\n",
    "all_counts: Counter = sum((counts[split] for split in counts), Counter())\n",
    "occurrences: List[str] = sorted(all_counts.values())\n",
    "\n",
    "plt.figure(figsize=(20, 15))\n",
    "plt.bar(range(len(occurrences)), occurrences)\n",
    "plt.xlim(0,len(occurrences))\n",
    "plt.ylim(1, 1e5)\n",
    "plt.yscale('log')\n",
    "plt.grid()\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}